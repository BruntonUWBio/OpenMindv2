{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "import h5py\n",
    "from simple_edf_preprocessing import Preprocessor\n",
    "from datetime import datetime\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a ECOG PCA class for its PCA object, hyperparas and other stuff\n",
    "class Feature_generator:\n",
    "    def __init__(self,path,pca_obj=None,wsize=30, prefiltered=True):\n",
    "        self.pca=pca_obj\n",
    "        #hyperpara in s for how large the time window should be on whihc we calculate our fourier trafo\n",
    "        self.wsize=wsize\n",
    "        #sampling frequency and last sample taken\n",
    "        df=h5py.File(path)\n",
    "        self.sfreq=int(df['f_sample'][()])\n",
    "        #TODO THIS IS ENTERED MANUALLY FOR DAY 4, PAT c46fd46!!! CHANGE TO READ FROM FILE ONCE AVAILBABLE\n",
    "        ### I think this is wrong...this somehow assumes we really start a 12AM, but we don't..\n",
    "#         start_sample=12839700 #calculated from 7h7m59s400ms\n",
    "#         end_sample=34437392 #calc from 19h7m54s783ms\n",
    "        ###so the actual portion of the data we want is this, in s\n",
    "        self.start=11\n",
    "        self.end=43205\n",
    "        #preprocess data\n",
    "        preprocessor=Preprocessor(df,start_sample=int(start*500),end_sample=int(end*500))\n",
    "        self.data,self.bad_chan,self.bad_idx=preprocessor.preprocess(prefiltered_sd_kurt=prefiltered)\n",
    "        self.data=self.data[self.bad_chan!=True]\n",
    "        #how many samples in this dataset?\n",
    "        self._bin_data()\n",
    "        self.pca=None\n",
    "            \n",
    "    \n",
    "    #this function restructures the data into a 3D structure, where each row presents a channel, each column one second\n",
    "    #and the depth is the amount of samples per seconds (sfreq). \n",
    "    #This is to discard seconds where bad_idx are present and to be on par with the labels in the end\n",
    "    #This function also creates a mask of bins to discard from the bad_idx array\n",
    "    def _bin_data(self):\n",
    "        #where to end?\n",
    "        self.data_bin=self.data[:,:self.end*self.sfreq].reshape(self.data.shape[0],self.end,self.sfreq)\n",
    "        self.mask_bin=np.all(self.bad_idx[:self.end*self.sfreq].reshape(self.end,self.sfreq),axis=1)\n",
    "\n",
    "      \n",
    "    def _standardize(self, data,ax=0):\n",
    "        self.data_mean=np.mean(data,axis=ax)\n",
    "        data_dem=data-self.data_mean\n",
    "        std=np.std(data,axis=ax)\n",
    "        data_stand=data_dem/std\n",
    "        #self.data_scal=1000000\n",
    "        #data_scal=self.data_scal*data_dem\n",
    "        #return data_scal\n",
    "        return data_stand\n",
    "    \n",
    "#this function caps at 150Hz, then bins the data in a logarithmic fashion to account for smaller psd values in higher freqs\n",
    "    def _bin_psd(self,fr,psd):\n",
    "        fr_trun=fr[fr<=150]\n",
    "        fr_total=len(fr_trun)\n",
    "        fr_bins=np.arange(int(np.log2(fr_total)+1))\n",
    "        #truncate everythin above 150Hz\n",
    "        psd=psd[:,fr<=150]\n",
    "        psd_bins=np.zeros((psd.shape[0],len(fr_bins)))\n",
    "        prev=0\n",
    "        max_psd_per_bin=np.exp2(fr_bins).astype('int')\n",
    "        prev=0\n",
    "        for b in fr_bins:\n",
    "            if (b==len(fr_bins) or max_psd_per_bin[b]>=psd.shape[1]):\n",
    "                psd_bins[:,b]+=np.sum(psd[:,prev:],axis=1)\n",
    "            else:\n",
    "                psd_bins[:,b]=np.sum(psd[:,prev:max_psd_per_bin[b]],axis=1)\n",
    "            prev=max_psd_per_bin[b]\n",
    "        return fr_bins, psd_bins\n",
    "\n",
    "        \n",
    "    #create matrix as follows:\n",
    "    #columns: channels, for each channel the 200 frequencies (0-200Hz) (hece freq*cha length) BUT BINNED logarithmically\n",
    "    #rows: Time steps\n",
    "    #resulting matrix is 2D, Time Stepsx(Freq*Channels)\n",
    "    #note that this matrix is prone to constant change. Save the current data as member variable\n",
    "    #NEW: Option to do this with a sliding window of length wsize\n",
    "    def _calc_features(self,time_sta,time_stp, sliding_window=False):\n",
    "        time_it=time_sta\n",
    "        while True:\n",
    "            stop=time_it+self.wsize\n",
    "            if stop>=self.data_bin.shape[1]-1:\n",
    "                print('Not enough data for set end %d. Returning all data that is available in given range.'% time_stp)\n",
    "                break\n",
    "                \n",
    "            #Note that each column is exactly one second.\n",
    "            #get data in range of ALL channels, applying the following mask to filter out seconds with a bad index\n",
    "            mask=np.ma.compressed(np.ma.masked_array(range(time_it,stop),mask=self.mask_bin[time_it:stop]))\n",
    "            curr_data=self.data_bin[:,mask,:].reshape(self.data.shape[0],-1)\n",
    "            \n",
    "            #is this thing empty? continue\n",
    "            if not curr_data.size:\n",
    "                if(sliding_window):\n",
    "                    time_it+=1\n",
    "                else:\n",
    "                    time_it+=self.wsize\n",
    "                if time_it+self.wsize >= time_stp:\n",
    "                    break\n",
    "                continue\n",
    "                \n",
    "            #welch method \n",
    "            fr,psd=signal.welch(curr_data,self.sfreq)\n",
    "            fr_bin,psd_bin=self._bin_psd(fr,psd)\n",
    "            if time_it==time_sta:\n",
    "                self.fr_bin=fr_bin\n",
    "                #first time. create first column, flatten w/o argument is row major \n",
    "                mat=psd_bin.flatten()\n",
    "            else:\n",
    "                #after, add column for each time step\n",
    "                mat=np.column_stack((mat,psd_bin.flatten()))\n",
    "            #sliding window?\n",
    "            if (sliding_window):\n",
    "                time_it+=1\n",
    "            else:\n",
    "                time_it+=self.wsize\n",
    "            if time_it+self.wsize >= time_stp:\n",
    "                break\n",
    "                \n",
    "        data_scal=self._standardize(mat.T)\n",
    "        self.curr_data=data_scal\n",
    "        return data_scal\n",
    "    \n",
    "    def vis_raw_data(self, start, stop, chans=None):\n",
    "        if chans is None:\n",
    "            chans=range(self.data.shape[0])\n",
    "        st=int(start*self.sfreq)\n",
    "        stp=int(stop*self.sfreq)\n",
    "        data=self.data[chans,st:stp]\n",
    "        for p in range(0,len(chans)-1):\n",
    "            plt.plot(data[p])\n",
    "        plt.show()\n",
    "    \n",
    "    def vis_welch_data(self,start,stop,chans=None):\n",
    "        #account for wsize\n",
    "        start=int(start/self.wsize)\n",
    "        stop=int(stop/self.wsize)\n",
    "        rem=self.curr_data[:,start:stop]\n",
    "        plt.imshow(rem,cmap='viridis',aspect='auto')\n",
    "        \n",
    "    def vis_pc(self):\n",
    "        if self.pca is None:\n",
    "            raise ValueError('PCA not set up yet. Please call setup_PCA first.')\n",
    "        for p in range(self.pca.n_components):\n",
    "            plt.plot(self.pca.transform(self.curr_data)[:,p])\n",
    "        plt.xlabel('Time (in w_size)')\n",
    "        plt.ylabel('PC Value')\n",
    "        plt.title('First %d principal components' % self.pca.n_components)\n",
    "        plt.show()\n",
    "\n",
    "    #get elbow curve. This also outputs the optimal n_components for the given desired explained variancce.\n",
    "    def __elbow_curve(self,datapart,expl_var_lim):\n",
    "        components = range(1, datapart.shape[1] + 1)\n",
    "        explained_variance = []\n",
    "        #till where?\n",
    "        lim=min(100, datapart.shape[1])\n",
    "        count=0\n",
    "        for component in tqdm(components[:lim]):\n",
    "            pca = PCA(n_components=component)\n",
    "            pca.fit(datapart)\n",
    "            expl_var=sum(pca.explained_variance_ratio_)\n",
    "            explained_variance.append(expl_var)\n",
    "            count+=1\n",
    "            if(expl_var>(expl_var_lim/100.)):\n",
    "                optimal_no_comps=count\n",
    "                break\n",
    "        if(explained_variance[-1:][0]<(expl_var_lim/100.)):\n",
    "            print('Could not explain more than %d %% of the variance. n_comps is set to match this. Consider increasing data range or lowering demanded explained variance' % expl_var*100)\n",
    "            optimal_no_comps=components[-1:]\n",
    "        sns_plot = sns.regplot(\n",
    "            x=np.array(components[:count]), y=explained_variance,\n",
    "            fit_reg=False).get_figure()\n",
    "        return optimal_no_comps\n",
    "    \n",
    "    def _setup_PCA(self,train,expl_variance):\n",
    "        if not train and self.pca is None:\n",
    "            raise ValueError('Train set has to be generated first, otherwise no principal axis available for data trafo.')\n",
    "        if train:\n",
    "            print('Setting up PCA on current data range...')\n",
    "            no_comps=self.__elbow_curve(self.curr_data,expl_variance)\n",
    "            self.pca=PCA(n_components=no_comps)\n",
    "            self.pca.fit(self.curr_data)\n",
    "        return self.pca.transform(self.curr_data)\n",
    "    \n",
    "    def generate_features(self,start=0,end=None, sliding_window=False, train=True,expl_variance=85):\n",
    "        self._calc_features(time_sta=start, time_stp=end, sliding_window=sliding_window)\n",
    "        princ_components=self._setup_PCA(train=train,expl_variance=expl_variance)\n",
    "        return princ_components\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97,)\n"
     ]
    }
   ],
   "source": [
    "pecog=Feature_generator('/data2/users/stepeter/Preprocessing/processed_cb46fd46_4.h5',prefiltered=False)\n",
    "\n",
    "# wut=pecog.generate_labels(0,30000)\n",
    "\n",
    "# print(wut.shape)\n",
    "\n",
    "# for p in range(pecog.pca.n_components):\n",
    "#     plt.plot(wut[:,p])\n",
    "# plt.xlabel('Time (in w_size)')\n",
    "# plt.ylabel('PC Value')\n",
    "# plt.title('First %d principal components' % pecog.pca.n_components)\n",
    "# plt.show()\n",
    "\n",
    "# pecog.vis_raw_data(0,30000,range(20))\n",
    "\n",
    "# #pecog.vis_raw_data(idx[0]-5,idx[1]+5)\n",
    "# #pecog.vbis_raw_data(0,idx[1]+400)\n",
    "# pecog.vis_welch_data(0,30000)\n",
    "\n",
    "\n",
    "\n",
    "# good_data=pecog.calc_data_mat(idx[1]+100,idx[1]+400)\n",
    "# pecog.pca.fit(good_data)\n",
    "# good_data_trafo=pecog.pca.transform(good_data)\n",
    "# print(good_data_trafo.shape)\n",
    "# print(good_data.shape)\n",
    "\n",
    "\n",
    "# print(good_data.shape)\n",
    "# print(good_data_trafo.shape)\n",
    "\n",
    "# comps=pecog.pca.components_\n",
    "# print(pecog.raw.info['ch_names'][28])\n",
    "# print(comps.shape)\n",
    "# comps=comps.reshape((127,-1,2))\n",
    "# print(np.argmax(comps[:,:,1],axis=1))\n",
    "# #plt.plot(comps[5:,5:,0].T)\n",
    "# plt.plot(comps[:,:,1].T)\n",
    "# plt.ylim(-0.005,0.005)\n",
    "\n",
    "# print(len(pecog.raw.info['chs']))\n",
    "\n",
    "# #print(data_trafo)\n",
    "# plt.plot(good_data_trafo[:,0])\n",
    "# plt.plot(good_data_trafo[:,1])\n",
    "# #plt.xlim(-0.00001,0.00001)\n",
    "# #plt.ylim(-0.00001,0.00001)\n",
    "\n",
    "# #max(data_trafo[:,1])-min(data_trafo[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
